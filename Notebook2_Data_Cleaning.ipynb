{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd230e6",
   "metadata": {},
   "source": [
    "# Notebook 2 — Data Cleaning\n",
    "\n",
    "## 2.1 Attribute Selection\n",
    "- Drop columns with >50% missing values\n",
    "- Keep attributes relevant to ML and the Department target\n",
    "\n",
    "## 2.2 Data Transformation / Standardization\n",
    "- Trim spaces\n",
    "\n",
    "## 2.3 Error Detection and Correction\n",
    "- Solving consistency issues found in the DQ assessment\n",
    "\n",
    "## 2.4 Missing Values Handling\n",
    "- Strategy: Title → \"Untitled\" ; Classification → ML imputation with a forest model; Other string attributes → \"Unknown\"; All numeric attributes are complete\n",
    "\n",
    "## 2.5 Deduplication\n",
    "- Removing exact duplicates found by not considering the unique columns\n",
    "\n",
    "## 2.6 Outlier detection\n",
    "- Performed outlier detection, but shouldn't lead to any useful discoveries\n",
    "\n",
    "## 2.7 Post-Cleaning DQ Re-Assessment\n",
    "- Show improved completeness, duplicate reduction, and distributional changes\n",
    "\n",
    "Notes: Implement the minimal, well-justified cleaning steps only; preserve interpretability for slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9868edc4",
   "metadata": {},
   "source": [
    "Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f86a760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MET_MUSEUM_OBJECTS = pd.read_csv(\"./met_museum_objects.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f74ac4",
   "metadata": {},
   "source": [
    "2.1 Attribute Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "af2b1fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We identify columns with more than 50% missing values first\n",
    "to_delete = []\n",
    "for c in MET_MUSEUM_OBJECTS.columns:\n",
    "    if MET_MUSEUM_OBJECTS[c].isnull().sum() > (MET_MUSEUM_OBJECTS.shape[0]*.5):\n",
    "        to_delete.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5abb29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are also dropping some more additional columns because they aren't useful for our purposes\n",
    "manual_deletions = [\"Metadata Date\", \"Repository\"] # Fully constant columns\n",
    "manual_deletions.append(\"Object Date\") # Redundant field already present in Object begin date and Object end date (which have 100% completeness)\n",
    "manual_deletions.append(\"Is Highlight\") # Extremely unbalanced column\n",
    "to_delete = to_delete + manual_deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3295152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we drop them\n",
    "MODIFIED_DATASET = MET_MUSEUM_OBJECTS.drop(columns=to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f227e75",
   "metadata": {},
   "source": [
    "2.2 Data Transformation / Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84613ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Object Name': 1192 values cleaned\n",
      "Column 'Title': 413 values cleaned\n",
      "Column 'Artist Display Name': 140 values cleaned\n",
      "Column 'Artist Alpha Sort': 2198 values cleaned\n",
      "Column 'Medium': 3295 values cleaned\n",
      "Column 'Dimensions': 7588 values cleaned\n",
      "Column 'Credit Line': 2944 values cleaned\n"
     ]
    }
   ],
   "source": [
    "# Removing possible initial and final whitespace from text objects\n",
    "text_cols = MODIFIED_DATASET.select_dtypes(include='object').columns\n",
    "\n",
    "# Loop for counting how many values are cleaned\n",
    "for col in text_cols:\n",
    "    has_whitespace = (MODIFIED_DATASET[col] != MODIFIED_DATASET[col].str.strip()) & MODIFIED_DATASET[col].notna()\n",
    "    count = has_whitespace.sum()\n",
    "    \n",
    "    if count > 0:\n",
    "        print(f\"Column '{col}': {count} values cleaned\")\n",
    "\n",
    "# Loop for actually deleting whitespaces\n",
    "for col in text_cols:\n",
    "    MODIFIED_DATASET[col] = MODIFIED_DATASET[col].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c36aa6",
   "metadata": {},
   "source": [
    "2.3 Error checking and solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bd97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before cleanup: 448203\n",
      "Cleanup complete. Rows remaining: 447971\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows with errors in dates as they are a limited amount and shouldn't affect the accuracy of the model\n",
    "\n",
    "# Initial length\n",
    "print(f\"Rows before cleanup: {len(MODIFIED_DATASET)}\")\n",
    "\n",
    "condition_good = (\n",
    "    (MODIFIED_DATASET['Object Begin Date'] <= MODIFIED_DATASET['Object End Date']) &\n",
    "    (MODIFIED_DATASET['Object Begin Date'] <= 2026) &\n",
    "    (MODIFIED_DATASET['Object End Date'] <= 2026)  # May delete some artists and objects that are still active or that haven't been completed yet\n",
    ")\n",
    "\n",
    "# Apply the filter\n",
    "MODIFIED_DATASET = MODIFIED_DATASET[condition_good]\n",
    "\n",
    "# Final length\n",
    "print(f\"Cleanup complete. Rows remaining: {len(MODIFIED_DATASET)}\")\n",
    "\n",
    "# Deleted around 230 rows in a 448000 row dataset (minor difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9a0aa",
   "metadata": {},
   "source": [
    "2.4 Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704cab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 447971\n",
      "Rows after removing content duplicates: 406324\n"
     ]
    }
   ],
   "source": [
    "# Exact duplicates aren't present because there are unique columns\n",
    "print(f\"Original rows: {len(MODIFIED_DATASET)}\")\n",
    "\n",
    "# Fuzzy duplicate detection\n",
    "# Check if by removing unique IDs there are duplicates\n",
    "unique_cols = ['Object ID', 'Object Number', 'Link Resource']\n",
    "\n",
    "# List of columns to check (Everything EXCEPT the unique ones)\n",
    "cols_to_check = [col for col in MODIFIED_DATASET.columns if col not in unique_cols]\n",
    "\n",
    "# Drop duplicates based on that subset\n",
    "MODIFIED_DATASET = MODIFIED_DATASET.drop_duplicates(subset=cols_to_check, keep='first')\n",
    "\n",
    "print(f\"Rows after removing content duplicates: {len(MODIFIED_DATASET)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292872ef",
   "metadata": {},
   "source": [
    "2.5 Missing values handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b3eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing Classification...\n",
      "Successfully imputed 48153 values for Classification\n"
     ]
    }
   ],
   "source": [
    "# Forest imputation for Classification\n",
    "\n",
    "missing_columns = [\"Classification\"] # Column to impute\n",
    "\n",
    "# Columns to exclude from the model\n",
    "exclude_cols = [\n",
    "    'Object ID', 'Object Number', 'Title', 'Link Resource', \n",
    "    'Artist Display Name', 'Artist Alpha Sort', 'Credit Line', \n",
    "    'Dimensions', 'Artist Role'\n",
    "]\n",
    "\n",
    "IMP_DATA = pd.DataFrame(columns=[\"IMP\" + name for name in missing_columns])\n",
    "\n",
    "# We fill missing values temporarily so they can be used as features if needed\n",
    "for feature in missing_columns:\n",
    "    MODIFIED_DATASET[feature + '_imp'] = MODIFIED_DATASET[feature]\n",
    "    mode_val = MODIFIED_DATASET[feature].mode()[0]\n",
    "    MODIFIED_DATASET.loc[MODIFIED_DATASET[feature].isnull(), feature + '_imp'] = mode_val\n",
    "\n",
    "# Random Forest Classification\n",
    "for feature in missing_columns:\n",
    "    print(f\"Imputing {feature}...\")\n",
    "    \n",
    "    IMP_DATA[\"IMP\" + feature] = MODIFIED_DATASET[feature]\n",
    "    \n",
    "    # Define predictors: All cols - Missing Cols - Excluded Cols\n",
    "    parameters = [c for c in MODIFIED_DATASET.columns \n",
    "                  if c not in missing_columns \n",
    "                  and c not in exclude_cols \n",
    "                  and not c.endswith('_imp')]\n",
    "\n",
    "\n",
    "    model = ensemble.RandomForestClassifier()\n",
    "\n",
    "    # Encode Variables\n",
    "    # Changed encoding from one hot to label so that it executes in a reasonable amount of space\n",
    "    X = pd.DataFrame(index=MODIFIED_DATASET.index)\n",
    "    for param in parameters:\n",
    "        if MODIFIED_DATASET[param].dtype == 'object' or MODIFIED_DATASET[param].dtype == 'bool':\n",
    "            # Convert text/bool to numbers (0, 1, 2...)\n",
    "            X[param], _ = pd.factorize(MODIFIED_DATASET[param])\n",
    "        else:\n",
    "            # Keep numbers as they are\n",
    "            X[param] = MODIFIED_DATASET[param].fillna(0) # Fill numeric NaNs with 0 for safety, but shouldn't fill anything as the numeric columns are complete\n",
    "    \n",
    "    # Define Train (Known values) and Predict (Missing values)\n",
    "    train_idx = MODIFIED_DATASET[feature].notnull()\n",
    "    predict_idx = MODIFIED_DATASET[feature].isnull()\n",
    "    \n",
    "    if predict_idx.sum() > 0:\n",
    "        # Fit model on known data\n",
    "        y_train = MODIFIED_DATASET.loc[train_idx, feature] # Target\n",
    "        X_train = X.loc[train_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict missing data\n",
    "        X_predict = X.loc[predict_idx]\n",
    "        model_predicted = model.predict(X_predict)\n",
    "\n",
    "        print(f\"Successfully imputed {len(model_predicted)} values for {feature}\")\n",
    "        IMP_DATA.loc[predict_idx, \"IMP\" + feature] = model_predicted\n",
    "    else:\n",
    "        print(f\"No missing values found for {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bd05a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading imputation results to the dataframe and removing temporary column\n",
    "MODIFIED_DATASET['Classification'] = IMP_DATA['IMPClassification']\n",
    "MODIFIED_DATASET = MODIFIED_DATASET.drop(columns='Classification_imp')\n",
    "\n",
    "# Column specific simple imputation\n",
    "MODIFIED_DATASET['Title'] = MODIFIED_DATASET['Title'].fillna('Untitled')\n",
    "\n",
    "# Unknown for other object fields\n",
    "text_cols = MODIFIED_DATASET.select_dtypes(include=['object']).columns\n",
    "\n",
    "MODIFIED_DATASET[text_cols] = MODIFIED_DATASET[text_cols].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c4bdcb",
   "metadata": {},
   "source": [
    "2.6 Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6bcc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The detected outliers are:  Series([], Name: Object Begin Date, dtype: int64)\n",
      "The detected outliers are:  Series([], Name: Object End Date, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "def IQR(data):\n",
    "    sorted(data)\n",
    "    Q1, Q3 = np.percentile(data, [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower_range = Q1 - (1.5 * IQR) # Leads to many outliers because of the unbalanced distribution of the data, most of which are normal values\n",
    "    upper_range = Q3 + (1.5 * IQR)\n",
    "    outliers = data[(data > upper_range)]\n",
    "    print(\"The detected outliers are: \", str(outliers))\n",
    "\n",
    "num_cols = ['Object Begin Date', 'Object End Date']\n",
    "for col in num_cols:\n",
    "    IQR(MODIFIED_DATASET[col])\n",
    "\n",
    "# MODIFIED IQR Outlier detection to not include lower outliers (museum data may be very old)\n",
    "# We don't get any useful information from this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5ea2b",
   "metadata": {},
   "source": [
    "Creation of cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5780729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODIFIED_DATASET.to_csv('cleaned_met_museum_objects.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
